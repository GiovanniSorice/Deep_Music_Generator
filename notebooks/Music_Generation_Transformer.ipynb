{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music_Generation_Transformer.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMMawPO9luVcjAw/uKCavpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Deep_Music_Generator/blob/main/notebooks/Music_Generation_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ3GwlYmvNT"
      },
      "source": [
        "# Transformer Music Generator "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNwQUwLIm7oW"
      },
      "source": [
        "\n",
        "\n",
        "In this notebook, we use an Transformer to generate some music.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFVGA5D4azA4"
      },
      "source": [
        "**This notebook was inspired (and part of the code comes from it) by [Music_Generation_LSTM](https://colab.research.google.com/drive/19TQqekOlnOSW36VCL8CPVEQKBBukmaEQ#scrollTo=DDOBVWULXfpz)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtmj0efInB_L"
      },
      "source": [
        "\n",
        "\n",
        "**Load dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_tzJeReygOv"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCsb9kaJmWHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b395dfa3-ea56-4653-98b1-296cf1f2e10a"
      },
      "source": [
        "pip install compressive_transformer_pytorch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting compressive_transformer_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/30/39/b8caf2671abcb8615977c08766aa9f450addd6949f57c7dda87224e844b5/compressive_transformer_pytorch-0.3.20-py3-none-any.whl\n",
            "Collecting mogrifier\n",
            "  Downloading https://files.pythonhosted.org/packages/77/01/62a55d0f8048e788fce435f2ade6478f443e4e53ed9b89b55ba0fc42c198/mogrifier-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from compressive_transformer_pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->compressive_transformer_pytorch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->compressive_transformer_pytorch) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->compressive_transformer_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->compressive_transformer_pytorch) (0.8)\n",
            "Installing collected packages: mogrifier, compressive-transformer-pytorch\n",
            "Successfully installed compressive-transformer-pytorch-0.3.20 mogrifier-0.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mi6KoBmX44A"
      },
      "source": [
        "import torch\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from compressive_transformer_pytorch import CompressiveTransformer\n",
        "from compressive_transformer_pytorch import AutoregressiveWrapper\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorflow.keras import utils\n",
        "from sklearn.metrics import roc_auc_score \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import pickle\n",
        "from music21 import converter, instrument, stream, note, chord\n",
        "import math\n",
        "import shutil"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwF2wx4oRZXW"
      },
      "source": [
        "# Set to false if you are not running\n",
        "# this notebook in Google Colaboratory\n",
        "run_on_colab = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2w9a2MknGmP"
      },
      "source": [
        "**Set hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnnLeO8Emsx3"
      },
      "source": [
        "# output directory name:\n",
        "output_dir = '/content/drive/My Drive/ISPR_project/Transformer/'\n",
        "current_path ='/content/drive/My Drive/ISPR_project/'\n",
        "# training:\n",
        "epochs = 2000\n",
        "batch_size = 64\n",
        "learning_rate=1e-2\n",
        "# vector-space embedding: \n",
        "n_dim = 64 \n",
        "sequence_length = 64\n",
        "\n",
        "\n",
        "VALIDATE_EVERY  = 5\n",
        "\n",
        "GENERATE_EVERY  = 500\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUFOx5yB39xx"
      },
      "source": [
        "**Save model function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQHDRGj838-0"
      },
      "source": [
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, output_dir+filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(output_dir+filename, output_dir+'model_best.pth.tar')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBjPl0cbI0_"
      },
      "source": [
        "**Google drive configuration (only Colab)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYmd3FoQq-Ri",
        "outputId": "9656b4b7-2898-49c0-c0b9-1cabf80a14fa"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Dec 15 09:12:19 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8    12W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKil2p6sM2gZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d81f5b62-d46a-4ad1-9374-9a9f10f28716"
      },
      "source": [
        "if(run_on_colab):\n",
        "  from google.colab import drive\n",
        "  # This will prompt for authorization.\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3gQinYmutQ"
      },
      "source": [
        "**Load data** \\\\\n",
        "Original MIDI files\n",
        " I have obtained  **MIDI files** from [The Lakh MIDI Dataset v0.1](https://colinraffel.com/projects/lmd/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wavgJZNpcR_f"
      },
      "source": [
        "## Processing data\n",
        "\n",
        "Let's process the files, and load them into **music21**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vILKBwYIcOvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b79edb-7607-4098-ef77-7301ea1e8c8f"
      },
      "source": [
        "file = current_path+\"midi_songs/dataset/Metal/Metallica/Am I Evil?.mid\"\n",
        "midi = converter.parse(file)\n",
        "notes_to_parse = midi.flat.notes\n",
        "for element in notes_to_parse[:10]:\n",
        "  print(element, element.offset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<music21.chord.Chord E2 E3 B3 E4> 0.0\n",
            "<music21.chord.Chord E2 E3 B3 E4> 0.0\n",
            "<music21.note.Note E> 0.0\n",
            "<music21.chord.Chord C2 C#3> 0.0\n",
            "<music21.note.Note G#> 2.0\n",
            "<music21.chord.Chord D3 A3 D4> 3.0\n",
            "<music21.chord.Chord D3 A3 D4> 3.0\n",
            "<music21.note.Note D> 3.0\n",
            "<music21.chord.Chord C#3 C2> 3.0\n",
            "<music21.chord.Chord B3 E3 E4> 3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjyrXfKic2kI"
      },
      "source": [
        "I will process all MIDI files obtaining data from each note of chord.\n",
        "\n",
        "- If I process a **note**, I will store in the list a string representing the pitch (the note name) and the octave.\n",
        "\n",
        "- If I process a **chord** (Remember that chords are set of notes that are played at the same time) I will store a different type of string with numbers separated by dots. Each number represents the pitch of a chord note. \n",
        "\n",
        "As you can see, **I are not considering yet time offsets of each element**. In this first version, we won't consider them, so all the notes and chords will have the same duration. Maybe, in the future, I will consider them.\n",
        "\n",
        "I are creating a big list with all the elements of all the compositions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iawbQHYVTOFM"
      },
      "source": [
        "notes_for_instruments = []\n",
        "for i,file in enumerate(glob.glob(current_path+\"midi_songs/dataset/*/*/*.mid\")):\n",
        "      midi = converter.parse(file)\n",
        "      print('Parsing file ', i, \" \", file)\n",
        "      notes_to_parse = None\n",
        "      try:  # file has instrument parts\n",
        "          s2 = instrument.partitionByInstrument(midi)\n",
        "          notes_to_parse = s2.recurse()\n",
        "      except:  # file has notes in a flat structure\n",
        "          notes_to_parse = midi.flat.notes\n",
        "      notes_instrument = []\n",
        "      for element in notes_to_parse:\n",
        "          if isinstance(element, note.Note):\n",
        "              notes_instrument.append(str(element.pitch))\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              notes_instrument.append('.'.join(str(n) for n in element.normalOrder))\n",
        "      notes_for_instruments.append(notes_instrument)\n",
        "with open(current_path + 'notes_for_instruments', 'wb') as filepath:\n",
        "    pickle.dump(notes_for_instruments, filepath)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj7MnrxIHLwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5dc237a-c9e6-4ea4-9030-2b11f4a6be1e"
      },
      "source": [
        "notes_for_instruments_validation = []\n",
        "for i,file in enumerate(glob.glob(current_path+\"midi_songs/validation/*.mid\")):\n",
        "      midi = converter.parse(file)\n",
        "      print('Parsing file ', i, \" \", file)\n",
        "      notes_to_parse = None\n",
        "      try:  # file has instrument parts\n",
        "          s2 = instrument.partitionByInstrument(midi)\n",
        "          notes_to_parse = s2.recurse()\n",
        "      except:  # file has notes in a flat structure\n",
        "          notes_to_parse = midi.flat.notes\n",
        "      notes_instrument = []\n",
        "      for element in notes_to_parse:\n",
        "          if isinstance(element, note.Note):\n",
        "              notes_instrument.append(str(element.pitch))\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              notes_instrument.append('.'.join(str(n) for n in element.normalOrder))\n",
        "      notes_for_instruments_validation.append(notes_instrument)\n",
        "with open(current_path + 'VALIDATION_notes_for_instruments', 'wb') as filepath:\n",
        "    pickle.dump(notes_for_instruments_validation, filepath)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing file  0   /content/drive/My Drive/ISPR_project/midi_songs/validation/Crazy Little Thing Called Love.mid\n",
            "Parsing file  1   /content/drive/My Drive/ISPR_project/midi_songs/validation/Nothing Else Matters.2.mid\n",
            "Parsing file  2   /content/drive/My Drive/ISPR_project/midi_songs/validation/King Nothing.1.mid\n",
            "Parsing file  3   /content/drive/My Drive/ISPR_project/midi_songs/validation/Fixxxer.mid\n",
            "Parsing file  4   /content/drive/My Drive/ISPR_project/midi_songs/validation/Motorbreath.mid\n",
            "Parsing file  5   /content/drive/My Drive/ISPR_project/midi_songs/validation/Porch.mid\n",
            "Parsing file  6   /content/drive/My Drive/ISPR_project/midi_songs/validation/A Kind of Magic.mid\n",
            "Parsing file  7   /content/drive/My Drive/ISPR_project/midi_songs/validation/Don't Chain My Heart.mid\n",
            "Parsing file  8   /content/drive/My Drive/ISPR_project/midi_songs/validation/Se tornerai.1.mid\n",
            "Parsing file  9   /content/drive/My Drive/ISPR_project/midi_songs/validation/Pamela.1.mid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr4zRlBMf-f5"
      },
      "source": [
        "with open(current_path + 'notes_for_instruments', 'rb') as f:\n",
        "    notes_for_instruments = pickle.load(f)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pmIFIu9f-4K"
      },
      "source": [
        "with open(current_path + 'VALIDATION_notes_for_instruments', 'rb') as f:\n",
        "    notes_for_instruments_validation = pickle.load(f)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuHePM_3dMXM"
      },
      "source": [
        "I obtain the number of different notes in our dataset, because this will be the **number of possible output classes**  of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEiySfEnTzyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae5bb28-d795-4a04-901b-a6f0a6d1579d"
      },
      "source": [
        "# Count different possible outputs\n",
        "print(len(set(item for notes_for_instrument in notes_for_instruments for item in notes_for_instrument)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khLas76ZHecL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07576fd6-0b2b-4e50-c780-67e5a4022069"
      },
      "source": [
        "# Count different possible outputs valifation\n",
        "print(len(set(item for notes_for_instrument in notes_for_instruments_validation for item in notes_for_instrument)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-mABhgUnTcV"
      },
      "source": [
        "**Preprocess data** \\\\\n",
        "Now, there is some **data processing** that I have to do:\n",
        "\n",
        "- I will map each pitch or chord to an integer\n",
        "- I will create pairs of input sequences and its corresponding output note\n",
        "\n",
        "I can try different **sequence_length** to obtain different results. In this first version, I will use a sequence_length of 100.\n",
        "\n",
        "The network will made its prediction of the next note (or chord), based on the previous *sequence_length* notes (or chords). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYkq3qb-hLra"
      },
      "source": [
        "# get all pitch names\n",
        "pitchnames_training = set(item for notes_for_instrument in notes_for_instruments for item in notes_for_instrument)\n",
        "pitchnames_validation = set(item for notes_for_instrument in notes_for_instruments_validation for item in notes_for_instrument)\n",
        "pitchnames = sorted(pitchnames_training.union(pitchnames_validation))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFVChPCk-enO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8960cde-3813-435d-8934-cd70877d0f5c"
      },
      "source": [
        "n_vocab = len(pitchnames)\n",
        "n_vocab"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "839"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4PpfIFRnWIA"
      },
      "source": [
        "# create a dictionary to map pitches to integers\n",
        "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "network_input = []\n",
        "for notes in notes_for_instruments:\n",
        "    if len(notes) - sequence_length<=0:\n",
        "        print(\"canzone troppo corta\")\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "      # Map pitches of sequence_in to integers\n",
        "      network_input.append([note_to_int[char] for char in notes[i:i + sequence_length]])\n",
        "n_patterns = len(network_input)\n",
        "# reshape the input into a format compatible with Transormer layers\n",
        "network_input = np.reshape(network_input, (n_patterns, sequence_length))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QIDFrdeIIz1"
      },
      "source": [
        "# create a dictionary to map pitches to integers\n",
        "note_to_int_validation = dict((notes_validation, number) for number, notes_validation in enumerate(pitchnames))\n",
        "network_input_validation = []\n",
        "network_output_validation = []\n",
        "for notes_validation in notes_for_instruments_validation:\n",
        "    if len(notes_validation) - sequence_length<=0:\n",
        "        print(\"canzone troppo corta\")\n",
        "    # create input sequences and the corresponding outputs\n",
        "    for i in range(0, len(notes_validation) - sequence_length, 1):\n",
        "      # Map pitches of sequence_in to integers\n",
        "      network_input_validation.append([note_to_int_validation[char] for char in notes_validation[i:i + sequence_length]])\n",
        "n_patterns_validation = len(network_input_validation)\n",
        "# reshape the input into a format compatible with Transormer layers\n",
        "network_input_validation = np.reshape(network_input_validation, (n_patterns_validation, sequence_length))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVRPauXUdq5Y"
      },
      "source": [
        "Let's see the new metwork_input size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8eQQMbxhUkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8de515-5261-44cd-f77d-0091ec7efceb"
      },
      "source": [
        "network_input.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(366889, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-c9sz4Bo8eZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f12116f-e58e-4f58-fcd7-25e49d57843e"
      },
      "source": [
        "network_input_validation.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36341, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6--Wc8UXnVgY"
      },
      "source": [
        "**Design neural network architecture** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4ztPAQsnb7T"
      },
      "source": [
        "def create_network(sequence_length, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = CompressiveTransformer(\n",
        "    num_tokens = n_vocab,\n",
        "    dim = sequence_length,\n",
        "    depth = 6,\n",
        "    seq_len = sequence_length,\n",
        "    mem_len = sequence_length,\n",
        "    cmem_len = 256,\n",
        "    cmem_ratio = 4,\n",
        "    memory_layers = [5,6],\n",
        "    gru_gated_residual = False\n",
        "    )\n",
        "\n",
        "    model = AutoregressiveWrapper(model)\n",
        "    model.cuda()\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nsNVMocnhNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9619aebb-756c-4332-a8a5-7a9528c775e8"
      },
      "source": [
        "model = create_network(sequence_length,n_vocab)\n",
        "\n",
        "print(model)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AutoregressiveWrapper(\n",
            "  (net): CompressiveTransformer(\n",
            "    (token_emb): Embedding(839, 64)\n",
            "    (to_model_dim): Identity()\n",
            "    (to_logits): Sequential(\n",
            "      (0): Identity()\n",
            "      (1): Linear(in_features=64, out_features=839, bias=True)\n",
            "    )\n",
            "    (attn_layers): ModuleList(\n",
            "      (0): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): SelfAttention(\n",
            "            (compress_mem_fn): ConvCompress(\n",
            "              (conv): Conv1d(64, 64, kernel_size=(4,), stride=(4,))\n",
            "            )\n",
            "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
            "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (reconstruction_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): SelfAttention(\n",
            "            (compress_mem_fn): ConvCompress(\n",
            "              (conv): Conv1d(64, 64, kernel_size=(4,), stride=(4,))\n",
            "            )\n",
            "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
            "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (reconstruction_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): SelfAttention(\n",
            "            (compress_mem_fn): ConvCompress(\n",
            "              (conv): Conv1d(64, 64, kernel_size=(4,), stride=(4,))\n",
            "            )\n",
            "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
            "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (reconstruction_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): SelfAttention(\n",
            "            (compress_mem_fn): ConvCompress(\n",
            "              (conv): Conv1d(64, 64, kernel_size=(4,), stride=(4,))\n",
            "            )\n",
            "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
            "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (reconstruction_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (4): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): SelfAttention(\n",
            "            (compress_mem_fn): ConvCompress(\n",
            "              (conv): Conv1d(64, 64, kernel_size=(4,), stride=(4,))\n",
            "            )\n",
            "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
            "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (reconstruction_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (5): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): SelfAttention(\n",
            "            (compress_mem_fn): ConvCompress(\n",
            "              (conv): Conv1d(64, 64, kernel_size=(4,), stride=(4,))\n",
            "            )\n",
            "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (to_kv): Linear(in_features=64, out_features=128, bias=False)\n",
            "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (reconstruction_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ff_layers): ModuleList(\n",
            "      (0): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (w1): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (act): GELU()\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (w2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (1): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (w1): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (act): GELU()\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (w2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (2): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (w1): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (act): GELU()\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (w2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (3): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (w1): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (act): GELU()\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (w2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (4): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (w1): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (act): GELU()\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (w2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (5): Residual(\n",
            "        (fn): PreNorm(\n",
            "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "          (fn): FeedForward(\n",
            "            (w1): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (act): GELU()\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (w2): Linear(in_features=256, out_features=64, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRKI1q5WjwcL"
      },
      "source": [
        "def cycle(loader):\n",
        "    while True:\n",
        "        for data in loader:\n",
        "          yield data\n",
        "\n",
        "data_train = torch.from_numpy(network_input).cuda()\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=32) \n",
        "cycle_train_loader  = cycle(DataLoader(data_train, batch_size = data_train.shape[0]))\n",
        "num_batches=math.ceil(data_train.shape[0]/batch_size) # Total number of batches"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFYgRHLRIy3C"
      },
      "source": [
        "#Validation\n",
        "data_validation = torch.from_numpy(network_input_validation).cuda()\n",
        "validation_loader = torch.utils.data.DataLoader(data_validation, batch_size=32) \n",
        "cycle_validation_loader  = cycle(DataLoader(data_validation, batch_size = data_validation.shape[0]))\n",
        "num_batches_val=math.ceil(data_validation.shape[0]/batch_size) # Total number of batches"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E60il158kxfq"
      },
      "source": [
        "# optimizer\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbO-XA3Dfrd2"
      },
      "source": [
        "In case we want to use previously trained weights, to continue the training in the point we left it, we should load them into the model.\n",
        "\n",
        "This is very useful in Google Colaboratory, that usually kills the virtual machine that is executing the Jupyter notework after a certime amount of time. If this happens to you, you should have to look for the last weights file in your configured Drive account and use it to train the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjfQL_Ck_92a"
      },
      "source": [
        "# In case we want to use previously trained weights\n",
        "weights = \"model_best.pth.tar\"\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/ISPR_project/Transformer/model_best.pth.tar\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqX3PsrJkyNN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0684b0ec-f5bf-48e8-ae65-e339b01663d1"
      },
      "source": [
        "# training\n",
        "for i in tqdm.tqdm(range(121,epochs), mininterval=20., desc='training'):\n",
        "    model.train()\n",
        "    tot_loss = 0.0\n",
        "    is_best=0\n",
        "    best_loss_value=n_vocab\n",
        "    avg_loss_val=0\n",
        "    for mlm_loss, aux_loss, is_last in model(next(cycle_train_loader), max_batch_size = batch_size, return_loss = True):\n",
        "        loss = mlm_loss + aux_loss\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        tot_loss+=loss;\n",
        "\n",
        "        if is_last:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    if i % VALIDATE_EVERY == 0 or i==epochs-1:\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "          for loss_val, aux_loss_val, is_last_val in model(next(cycle_validation_loader), max_batch_size = batch_size, return_loss = True):\n",
        "            avg_loss_val+=loss_val/num_batches_val;\n",
        "\n",
        "            if is_last_val:\n",
        "              print(f'\\n validation loss: {avg_loss_val.item():.4f}')\n",
        "\n",
        "\n",
        "    avg_loss=tot_loss/num_batches\n",
        "\n",
        "    if i%5==0 or i==epochs-1:\n",
        "      if best_loss_value>avg_loss:\n",
        "        best_loss_value=avg_loss;\n",
        "        is_best=1\n",
        "\n",
        "      save_checkpoint({\n",
        "      'epoch': i,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict' : optimizer.state_dict(),\n",
        "      'loss':avg_loss.item(),\n",
        "     }, is_best, 'Tran_64_Checkpoint'+str(i)+'_'+\"{:.4f}\".format(avg_loss.item())+'.pth.tar')\n",
        "      is_best=0\n",
        "    print(f'\\n Epoch: {i} |Training loss: {avg_loss.item():.4f}')\n",
        "print('\\nTraining complete.')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training:   0%|          | 1/1879 [04:54<153:41:14, 294.61s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 121 |Training loss: 4.1223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 2/1879 [09:48<153:27:40, 294.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 122 |Training loss: 4.1109\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 3/1879 [14:45<153:53:33, 295.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 123 |Training loss: 4.0997\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 4/1879 [19:40<153:38:52, 295.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 124 |Training loss: 4.0885\n",
            "\n",
            " validation loss: 4.2872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 5/1879 [24:45<155:12:13, 298.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 125 |Training loss: 4.0774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 6/1879 [29:39<154:27:49, 296.89s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 126 |Training loss: 4.0664\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 7/1879 [34:33<153:55:34, 296.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 127 |Training loss: 4.0555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 8/1879 [39:23<152:53:33, 294.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 128 |Training loss: 4.0446\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   0%|          | 9/1879 [44:10<151:44:34, 292.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 129 |Training loss: 4.0339\n",
            "\n",
            " validation loss: 4.2378\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 10/1879 [49:06<152:16:08, 293.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 130 |Training loss: 4.0232\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 11/1879 [53:52<150:55:31, 290.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 131 |Training loss: 4.0126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 12/1879 [58:36<149:54:00, 289.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 132 |Training loss: 4.0020\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 13/1879 [1:03:22<149:17:03, 288.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 133 |Training loss: 3.9916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 14/1879 [1:08:08<148:50:45, 287.32s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 134 |Training loss: 3.9812\n",
            "\n",
            " validation loss: 4.1921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 15/1879 [1:13:08<150:44:53, 291.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 135 |Training loss: 3.9710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 16/1879 [1:17:57<150:20:36, 290.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 136 |Training loss: 3.9608\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 17/1879 [1:22:45<149:55:18, 289.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 137 |Training loss: 3.9507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 18/1879 [1:27:33<149:29:45, 289.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 138 |Training loss: 3.9408\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 19/1879 [1:32:20<149:09:09, 288.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 139 |Training loss: 3.9309\n",
            "\n",
            " validation loss: 4.1504\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 20/1879 [1:37:18<150:26:21, 291.33s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 140 |Training loss: 3.9212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 21/1879 [1:42:05<149:44:32, 290.14s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 141 |Training loss: 3.9116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 22/1879 [1:46:50<148:54:06, 288.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 142 |Training loss: 3.9021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|          | 23/1879 [1:51:36<148:19:39, 287.70s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 143 |Training loss: 3.8927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|▏         | 24/1879 [1:56:20<147:45:38, 286.76s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 144 |Training loss: 3.8834\n",
            "\n",
            " validation loss: 4.1121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|▏         | 25/1879 [2:01:16<149:07:49, 289.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 145 |Training loss: 3.8743\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|▏         | 26/1879 [2:06:02<148:21:35, 288.23s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 146 |Training loss: 3.8652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|▏         | 27/1879 [2:10:47<147:47:33, 287.29s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 147 |Training loss: 3.8563\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   1%|▏         | 28/1879 [2:15:32<147:29:30, 286.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 148 |Training loss: 3.8475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 29/1879 [2:20:18<147:08:56, 286.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 149 |Training loss: 3.8388\n",
            "\n",
            " validation loss: 4.0762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 30/1879 [2:25:14<148:34:35, 289.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 150 |Training loss: 3.8302\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 31/1879 [2:29:59<147:54:29, 288.13s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 151 |Training loss: 3.8217\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 32/1879 [2:34:44<147:20:34, 287.19s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 152 |Training loss: 3.8133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 33/1879 [2:39:29<146:56:24, 286.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 153 |Training loss: 3.8050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 34/1879 [2:44:14<146:36:58, 286.08s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 154 |Training loss: 3.7968\n",
            "\n",
            " validation loss: 4.0420\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 35/1879 [2:49:12<148:19:18, 289.57s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 155 |Training loss: 3.7887\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 36/1879 [2:53:59<147:49:56, 288.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 156 |Training loss: 3.7807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 37/1879 [2:58:46<147:27:02, 288.18s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 157 |Training loss: 3.7727\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 38/1879 [3:03:31<146:58:42, 287.41s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 158 |Training loss: 3.7649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 39/1879 [3:08:17<146:35:09, 286.80s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 159 |Training loss: 3.7571\n",
            "\n",
            " validation loss: 4.0096\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 40/1879 [3:13:13<147:59:57, 289.72s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 160 |Training loss: 3.7494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 41/1879 [3:17:59<147:22:59, 288.67s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 161 |Training loss: 3.7418\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 42/1879 [3:22:45<146:50:33, 287.77s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 162 |Training loss: 3.7342\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 43/1879 [3:27:31<146:32:44, 287.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 163 |Training loss: 3.7267\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 44/1879 [3:32:17<146:15:15, 286.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 164 |Training loss: 3.7193\n",
            "\n",
            " validation loss: 3.9791\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 45/1879 [3:37:15<147:45:34, 290.04s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 165 |Training loss: 3.7120\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   2%|▏         | 46/1879 [3:42:03<147:25:16, 289.53s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 166 |Training loss: 3.7047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 47/1879 [3:46:51<147:02:04, 288.93s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 167 |Training loss: 3.6974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 48/1879 [3:51:38<146:45:14, 288.54s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 168 |Training loss: 3.6903\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 49/1879 [3:56:25<146:28:41, 288.15s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 169 |Training loss: 3.6832\n",
            "\n",
            " validation loss: 3.9503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 50/1879 [4:01:25<148:04:46, 291.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 170 |Training loss: 3.6762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 51/1879 [4:06:11<147:15:47, 290.01s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 171 |Training loss: 3.6692\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 52/1879 [4:10:59<146:50:37, 289.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 172 |Training loss: 3.6623\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 53/1879 [4:15:48<146:42:55, 289.25s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 173 |Training loss: 3.6554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 54/1879 [4:20:37<146:38:22, 289.26s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 174 |Training loss: 3.6486\n",
            "\n",
            " validation loss: 3.9229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 55/1879 [4:25:42<148:53:59, 293.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 175 |Training loss: 3.6419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 56/1879 [4:30:34<148:27:02, 293.16s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 176 |Training loss: 3.6352\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 57/1879 [4:35:24<148:01:41, 292.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 177 |Training loss: 3.6286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 58/1879 [4:40:13<147:22:08, 291.34s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 178 |Training loss: 3.6220\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 59/1879 [4:45:02<146:53:46, 290.56s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 179 |Training loss: 3.6154\n",
            "\n",
            " validation loss: 3.8967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 60/1879 [4:50:00<147:58:26, 292.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 180 |Training loss: 3.6090\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 61/1879 [4:54:46<146:49:37, 290.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 181 |Training loss: 3.6025\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 62/1879 [4:59:33<146:09:45, 289.59s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 182 |Training loss: 3.5961\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 63/1879 [5:04:21<145:49:45, 289.09s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 183 |Training loss: 3.5898\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 64/1879 [5:09:08<145:26:46, 288.49s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 184 |Training loss: 3.5835\n",
            "\n",
            " validation loss: 3.8718\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   3%|▎         | 65/1879 [5:14:06<146:49:42, 291.39s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 185 |Training loss: 3.5773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▎         | 66/1879 [5:18:53<146:06:31, 290.12s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 186 |Training loss: 3.5711\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▎         | 67/1879 [5:23:39<145:27:47, 289.00s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 187 |Training loss: 3.5649\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▎         | 68/1879 [5:28:26<145:03:26, 288.35s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 188 |Training loss: 3.5588\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▎         | 69/1879 [5:33:13<144:44:15, 287.88s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 189 |Training loss: 3.5527\n",
            "\n",
            " validation loss: 3.8481\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▎         | 70/1879 [5:38:11<146:11:15, 290.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 190 |Training loss: 3.5467\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 71/1879 [5:42:59<145:40:16, 290.05s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 191 |Training loss: 3.5407\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 72/1879 [5:47:47<145:19:30, 289.52s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 192 |Training loss: 3.5348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 73/1879 [5:52:39<145:37:29, 290.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 193 |Training loss: 3.5288\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 74/1879 [5:57:27<145:06:55, 289.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 194 |Training loss: 3.5230\n",
            "\n",
            " validation loss: 3.8257\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 75/1879 [6:02:25<146:18:09, 291.96s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 195 |Training loss: 3.5171\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 76/1879 [6:07:12<145:28:16, 290.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 196 |Training loss: 3.5113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 77/1879 [6:12:00<145:02:17, 289.75s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 197 |Training loss: 3.5056\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 78/1879 [6:16:46<144:28:27, 288.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 198 |Training loss: 3.4998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 79/1879 [6:21:32<143:56:13, 287.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 199 |Training loss: 3.4941\n",
            "\n",
            " validation loss: 3.8044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 80/1879 [6:26:31<145:33:26, 291.28s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 200 |Training loss: 3.4885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 81/1879 [6:31:20<145:04:01, 290.46s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 201 |Training loss: 3.4829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 82/1879 [6:36:07<144:28:09, 289.42s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 202 |Training loss: 3.4773\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 83/1879 [6:40:54<144:00:35, 288.66s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 203 |Training loss: 3.4717\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   4%|▍         | 84/1879 [6:45:41<143:39:05, 288.10s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 204 |Training loss: 3.4662\n",
            "\n",
            " validation loss: 3.7843\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   5%|▍         | 85/1879 [6:50:39<145:08:50, 291.27s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 205 |Training loss: 3.4607\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   5%|▍         | 86/1879 [6:55:34<145:37:53, 292.40s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 206 |Training loss: 3.4552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\rtraining:   5%|▍         | 87/1879 [7:00:32<146:24:02, 294.11s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch: 207 |Training loss: 3.4498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAgrzFyRn3uq"
      },
      "source": [
        "**Music generation**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFbSthkLUwmx"
      },
      "source": [
        "# In case we want to use previously trained weights\n",
        "weights = \"model_best.pth.tar\"\n",
        "checkpoint = torch.load(output_dir+weights)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WtYEXpyoPPK"
      },
      "source": [
        "# Generate network input again\n",
        "network_input = []\n",
        "network_output = []\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "  network_input.append([note_to_int[char] for char in notes[i:i + sequence_length]])\n",
        "n_patterns = len(network_input)\n",
        "network_input = np.reshape(network_input, (n_patterns, sequence_length))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuUGi5rVgbJQ"
      },
      "source": [
        "The workflow now is:\n",
        "\n",
        "\n",
        "1.   Pick a **seed sequence** randomly from your list of inputs (*pattern* variable)\n",
        "2.   Pass it as input for your model to generate a new element (note or chord)\n",
        "3.   Add the new element to your final song and to your *pattern* list\n",
        "4.   Remove the first item from *pattern*\n",
        "5.   Go to step 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUTEiS0NXtkl"
      },
      "source": [
        "\"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "# pick a random sequence from the input as a starting point for the prediction\n",
        "start = np.random.randint(0, len(network_input)-1)\n",
        "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "pattern = torch.from_numpy(network_input[start]).cuda()\n",
        "\n",
        "prediction_output = model.generate(pattern, 500)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUlgtok68w-w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e5af720-53ab-4292-94da-8ef9be8aabfc"
      },
      "source": [
        "result_sample=[]\n",
        "\n",
        "for i in range(500):\n",
        "  print(i)\n",
        "  result = int_to_note[prediction_output[i].item()]\n",
        "  print('\\r', 'Predicted ', i, \" \",result, end='')\n",
        "  result_sample.append(result)\n",
        "\n",
        "prediction_output=result_sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "\r Predicted  0   61\n",
            "\r Predicted  1   4.62\n",
            "\r Predicted  2   6.113\n",
            "\r Predicted  3   64\n",
            "\r Predicted  4   6.115\n",
            "\r Predicted  5   A46\n",
            "\r Predicted  6   4.67\n",
            "\r Predicted  7   F48\n",
            "\r Predicted  8   69\n",
            "\r Predicted  9   610\n",
            "\r Predicted  10   5.7.9.011\n",
            "\r Predicted  11   2.3.7.1012\n",
            "\r Predicted  12   D513\n",
            "\r Predicted  13   C514\n",
            "\r Predicted  14   5.7.9.015\n",
            "\r Predicted  15   C516\n",
            "\r Predicted  16   4.617\n",
            "\r Predicted  17   B-118\n",
            "\r Predicted  18   10.2.519\n",
            "\r Predicted  19   C520\n",
            "\r Predicted  20   6.1121\n",
            "\r Predicted  21   622\n",
            "\r Predicted  22   F223\n",
            "\r Predicted  23   6.1124\n",
            "\r Predicted  24   4.625\n",
            "\r Predicted  25   B-226\n",
            "\r Predicted  26   B-127\n",
            "\r Predicted  27   A428\n",
            "\r Predicted  28   629\n",
            "\r Predicted  29   C530\n",
            "\r Predicted  30   E-331\n",
            "\r Predicted  31   F232\n",
            "\r Predicted  32   4.633\n",
            "\r Predicted  33   534\n",
            "\r Predicted  34   5.1035\n",
            "\r Predicted  35   4.636\n",
            "\r Predicted  36   637\n",
            "\r Predicted  37   4.638\n",
            "\r Predicted  38   4.639\n",
            "\r Predicted  39   F240\n",
            "\r Predicted  40   4.641\n",
            "\r Predicted  41   B-242\n",
            "\r Predicted  42   643\n",
            "\r Predicted  43   5.7.9.044\n",
            "\r Predicted  44   6.1145\n",
            "\r Predicted  45   646\n",
            "\r Predicted  46   B-247\n",
            "\r Predicted  47   A448\n",
            "\r Predicted  48   649\n",
            "\r Predicted  49   D550\n",
            "\r Predicted  50   C551\n",
            "\r Predicted  51   652\n",
            "\r Predicted  52   6.1153\n",
            "\r Predicted  53   B-254\n",
            "\r Predicted  54   655\n",
            "\r Predicted  55   10.2.556\n",
            "\r Predicted  56   4.657\n",
            "\r Predicted  57   5.9.058\n",
            "\r Predicted  58   5.9.059\n",
            "\r Predicted  59   4.660\n",
            "\r Predicted  60   E261\n",
            "\r Predicted  61   F262\n",
            "\r Predicted  62   4.663\n",
            "\r Predicted  63   F264\n",
            "\r Predicted  64   6.1165\n",
            "\r Predicted  65   5.9.066\n",
            "\r Predicted  66   4.667\n",
            "\r Predicted  67   10.2.568\n",
            "\r Predicted  68   A469\n",
            "\r Predicted  69   B-270\n",
            "\r Predicted  70   6.1171\n",
            "\r Predicted  71   B-272\n",
            "\r Predicted  72   6.1073\n",
            "\r Predicted  73   A474\n",
            "\r Predicted  74   F275\n",
            "\r Predicted  75   D576\n",
            "\r Predicted  76   A477\n",
            "\r Predicted  77   6.1178\n",
            "\r Predicted  78   679\n",
            "\r Predicted  79   6.1180\n",
            "\r Predicted  80   6.1081\n",
            "\r Predicted  81   682\n",
            "\r Predicted  82   5.7.9.083\n",
            "\r Predicted  83   6.1184\n",
            "\r Predicted  84   6.1085\n",
            "\r Predicted  85   B-286\n",
            "\r Predicted  86   687\n",
            "\r Predicted  87   6.1188\n",
            "\r Predicted  88   F489\n",
            "\r Predicted  89   B-290\n",
            "\r Predicted  90   D591\n",
            "\r Predicted  91   D592\n",
            "\r Predicted  92   4.693\n",
            "\r Predicted  93   B-294\n",
            "\r Predicted  94   4.695\n",
            "\r Predicted  95   10.2.596\n",
            "\r Predicted  96   B-297\n",
            "\r Predicted  97   C598\n",
            "\r Predicted  98   E-399\n",
            "\r Predicted  99   5.10100\n",
            "\r Predicted  100   4.6101\n",
            "\r Predicted  101   4.6102\n",
            "\r Predicted  102   A4103\n",
            "\r Predicted  103   5.9.0104\n",
            "\r Predicted  104   6.11105\n",
            "\r Predicted  105   F2106\n",
            "\r Predicted  106   C5107\n",
            "\r Predicted  107   6108\n",
            "\r Predicted  108   6.11109\n",
            "\r Predicted  109   5.9.0110\n",
            "\r Predicted  110   5.9.0111\n",
            "\r Predicted  111   A4112\n",
            "\r Predicted  112   6113\n",
            "\r Predicted  113   6.11114\n",
            "\r Predicted  114   A4115\n",
            "\r Predicted  115   A4116\n",
            "\r Predicted  116   6117\n",
            "\r Predicted  117   6.11118\n",
            "\r Predicted  118   5.9.0119\n",
            "\r Predicted  119   4.6120\n",
            "\r Predicted  120   E-5121\n",
            "\r Predicted  121   F2122\n",
            "\r Predicted  122   6.11123\n",
            "\r Predicted  123   4.6124\n",
            "\r Predicted  124   5125\n",
            "\r Predicted  125   4.6126\n",
            "\r Predicted  126   5127\n",
            "\r Predicted  127   4.6128\n",
            "\r Predicted  128   10.2.5129\n",
            "\r Predicted  129   A4130\n",
            "\r Predicted  130   6.10131\n",
            "\r Predicted  131   6.11132\n",
            "\r Predicted  132   6.11133\n",
            "\r Predicted  133   D5134\n",
            "\r Predicted  134   4.6135\n",
            "\r Predicted  135   E-2136\n",
            "\r Predicted  136   3.7.10137\n",
            "\r Predicted  137   7.10.2138\n",
            "\r Predicted  138   6139\n",
            "\r Predicted  139   6.11140\n",
            "\r Predicted  140   C5141\n",
            "\r Predicted  141   5.9.0142\n",
            "\r Predicted  142   6143\n",
            "\r Predicted  143   D5144\n",
            "\r Predicted  144   A4145\n",
            "\r Predicted  145   4.6146\n",
            "\r Predicted  146   E-5147\n",
            "\r Predicted  147   6148\n",
            "\r Predicted  148   6149\n",
            "\r Predicted  149   D5150\n",
            "\r Predicted  150   6.11151\n",
            "\r Predicted  151   A4152\n",
            "\r Predicted  152   E-2153\n",
            "\r Predicted  153   F4154\n",
            "\r Predicted  154   E-5155\n",
            "\r Predicted  155   6.11156\n",
            "\r Predicted  156   F2157\n",
            "\r Predicted  157   6158\n",
            "\r Predicted  158   F4159\n",
            "\r Predicted  159   F2160\n",
            "\r Predicted  160   6.11161\n",
            "\r Predicted  161   F2162\n",
            "\r Predicted  162   4.6163\n",
            "\r Predicted  163   5.9.0164\n",
            "\r Predicted  164   6.11165\n",
            "\r Predicted  165   E-2166\n",
            "\r Predicted  166   6.11167\n",
            "\r Predicted  167   2.5168\n",
            "\r Predicted  168   6.11169\n",
            "\r Predicted  169   6170\n",
            "\r Predicted  170   5.7.9.0171\n",
            "\r Predicted  171   F2172\n",
            "\r Predicted  172   6173\n",
            "\r Predicted  173   F4174\n",
            "\r Predicted  174   6.11175\n",
            "\r Predicted  175   E-2176\n",
            "\r Predicted  176   F2177\n",
            "\r Predicted  177   6178\n",
            "\r Predicted  178   7.10.2179\n",
            "\r Predicted  179   F2180\n",
            "\r Predicted  180   B-1181\n",
            "\r Predicted  181   C4182\n",
            "\r Predicted  182   6183\n",
            "\r Predicted  183   2.3.7.10184\n",
            "\r Predicted  184   0.5185\n",
            "\r Predicted  185   5.7.9.0186\n",
            "\r Predicted  186   6.11187\n",
            "\r Predicted  187   5.9.0188\n",
            "\r Predicted  188   C5189\n",
            "\r Predicted  189   E-3190\n",
            "\r Predicted  190   5.10191\n",
            "\r Predicted  191   6.11192\n",
            "\r Predicted  192   F2193\n",
            "\r Predicted  193   6194\n",
            "\r Predicted  194   10.11.3.6195\n",
            "\r Predicted  195   5196\n",
            "\r Predicted  196   5.10197\n",
            "\r Predicted  197   4.6198\n",
            "\r Predicted  198   4.6199\n",
            "\r Predicted  199   4.6200\n",
            "\r Predicted  200   5.9.0201\n",
            "\r Predicted  201   6.11202\n",
            "\r Predicted  202   D5203\n",
            "\r Predicted  203   D5204\n",
            "\r Predicted  204   6.11205\n",
            "\r Predicted  205   6206\n",
            "\r Predicted  206   D5207\n",
            "\r Predicted  207   D5208\n",
            "\r Predicted  208   6.11209\n",
            "\r Predicted  209   6210\n",
            "\r Predicted  210   F4211\n",
            "\r Predicted  211   6212\n",
            "\r Predicted  212   10.11.3.6213\n",
            "\r Predicted  213   6.11214\n",
            "\r Predicted  214   C5215\n",
            "\r Predicted  215   3.6.10216\n",
            "\r Predicted  216   6.11217\n",
            "\r Predicted  217   6218\n",
            "\r Predicted  218   4.6219\n",
            "\r Predicted  219   E2220\n",
            "\r Predicted  220   6.11221\n",
            "\r Predicted  221   6222\n",
            "\r Predicted  222   10.2.5223\n",
            "\r Predicted  223   6224\n",
            "\r Predicted  224   4.6225\n",
            "\r Predicted  225   B-1226\n",
            "\r Predicted  226   6.11227\n",
            "\r Predicted  227   5.9.0228\n",
            "\r Predicted  228   4.6229\n",
            "\r Predicted  229   4.6230\n",
            "\r Predicted  230   9.2231\n",
            "\r Predicted  231   10.2.5232\n",
            "\r Predicted  232   F2233\n",
            "\r Predicted  233   6.11234\n",
            "\r Predicted  234   6.10235\n",
            "\r Predicted  235   6.11236\n",
            "\r Predicted  236   A4237\n",
            "\r Predicted  237   B-2238\n",
            "\r Predicted  238   6239\n",
            "\r Predicted  239   6240\n",
            "\r Predicted  240   6.10.11241\n",
            "\r Predicted  241   6.11242\n",
            "\r Predicted  242   B-2243\n",
            "\r Predicted  243   C5244\n",
            "\r Predicted  244   6245\n",
            "\r Predicted  245   7.10.2246\n",
            "\r Predicted  246   D5247\n",
            "\r Predicted  247   4.6248\n",
            "\r Predicted  248   4.6249\n",
            "\r Predicted  249   6250\n",
            "\r Predicted  250   6.11251\n",
            "\r Predicted  251   F2252\n",
            "\r Predicted  252   4.6253\n",
            "\r Predicted  253   9.2254\n",
            "\r Predicted  254   D5255\n",
            "\r Predicted  255   4.6256\n",
            "\r Predicted  256   B-2257\n",
            "\r Predicted  257   6258\n",
            "\r Predicted  258   6.10.11259\n",
            "\r Predicted  259   4.6260\n",
            "\r Predicted  260   4.6261\n",
            "\r Predicted  261   4.6262\n",
            "\r Predicted  262   6263\n",
            "\r Predicted  263   6.11264\n",
            "\r Predicted  264   F2265\n",
            "\r Predicted  265   A4266\n",
            "\r Predicted  266   6267\n",
            "\r Predicted  267   D5268\n",
            "\r Predicted  268   6.11269\n",
            "\r Predicted  269   D5270\n",
            "\r Predicted  270   6271\n",
            "\r Predicted  271   6.11272\n",
            "\r Predicted  272   B-2273\n",
            "\r Predicted  273   6274\n",
            "\r Predicted  274   6.11275\n",
            "\r Predicted  275   C5276\n",
            "\r Predicted  276   B-2277\n",
            "\r Predicted  277   A4278\n",
            "\r Predicted  278   D5279\n",
            "\r Predicted  279   0.5280\n",
            "\r Predicted  280   E-2281\n",
            "\r Predicted  281   4.6282\n",
            "\r Predicted  282   4.6283\n",
            "\r Predicted  283   E-5284\n",
            "\r Predicted  284   6.11285\n",
            "\r Predicted  285   F2286\n",
            "\r Predicted  286   5.9.0287\n",
            "\r Predicted  287   4.6288\n",
            "\r Predicted  288   E-2289\n",
            "\r Predicted  289   10.2.5290\n",
            "\r Predicted  290   6291\n",
            "\r Predicted  291   F2292\n",
            "\r Predicted  292   6293\n",
            "\r Predicted  293   E-2294\n",
            "\r Predicted  294   3.7.10295\n",
            "\r Predicted  295   6296\n",
            "\r Predicted  296   6.11297\n",
            "\r Predicted  297   4.6298\n",
            "\r Predicted  298   B-1299\n",
            "\r Predicted  299   6.11300\n",
            "\r Predicted  300   5.9.0301\n",
            "\r Predicted  301   4.6302\n",
            "\r Predicted  302   5.9.0303\n",
            "\r Predicted  303   A4304\n",
            "\r Predicted  304   C5305\n",
            "\r Predicted  305   6306\n",
            "\r Predicted  306   F4307\n",
            "\r Predicted  307   E-3308\n",
            "\r Predicted  308   4.6309\n",
            "\r Predicted  309   5.9.0310\n",
            "\r Predicted  310   6.11311\n",
            "\r Predicted  311   6.10312\n",
            "\r Predicted  312   D5313\n",
            "\r Predicted  313   A4314\n",
            "\r Predicted  314   4.6315\n",
            "\r Predicted  315   E2316\n",
            "\r Predicted  316   10.2.5317\n",
            "\r Predicted  317   F2318\n",
            "\r Predicted  318   C5319\n",
            "\r Predicted  319   5.7.9.0320\n",
            "\r Predicted  320   6.11321\n",
            "\r Predicted  321   4.6322\n",
            "\r Predicted  322   A4323\n",
            "\r Predicted  323   10.2.5324\n",
            "\r Predicted  324   5.9.0325\n",
            "\r Predicted  325   6326\n",
            "\r Predicted  326   4.6327\n",
            "\r Predicted  327   5.9.0328\n",
            "\r Predicted  328   6.11329\n",
            "\r Predicted  329   5330\n",
            "\r Predicted  330   6.10331\n",
            "\r Predicted  331   F2332\n",
            "\r Predicted  332   6.11333\n",
            "\r Predicted  333   6334\n",
            "\r Predicted  334   6.11335\n",
            " Predicted  335   6336\n",
            " Predicted  336   E-2337\n",
            " Predicted  337   6.11338\n",
            " Predicted  338   B-2339\n",
            " Predicted  339   6340\n",
            " Predicted  340   5.7.9.0341\n",
            " Predicted  341   4.6342\n",
            " Predicted  342   4.6343\n",
            " Predicted  343   B-1344\n",
            " Predicted  344   6.11345\n",
            " Predicted  345   B-2346\n",
            " Predicted  346   6347\n",
            " Predicted  347   F4348\n",
            " Predicted  348   6.11349\n",
            " Predicted  349   B-2350\n",
            " Predicted  350   D5351\n",
            " Predicted  351   A4352\n",
            " Predicted  352   C5353\n",
            " Predicted  353   F2354\n",
            " Predicted  354   6.11355\n",
            " Predicted  355   6.11356\n",
            " Predicted  356   D5357\n",
            " Predicted  357   4.6358\n",
            " Predicted  358   5.9.0359\n",
            " Predicted  359   4.6360\n",
            " Predicted  360   E-5361\n",
            " Predicted  361   4.6362\n",
            " Predicted  362   6.11363\n",
            " Predicted  363   B-2364\n",
            " Predicted  364   6365\n",
            " Predicted  365   6366\n",
            " Predicted  366   E-2367\n",
            " Predicted  367   B-1368\n",
            " Predicted  368   6.11369\n",
            " Predicted  369   6370\n",
            " Predicted  370   5.7.9.0371\n",
            " Predicted  371   C5372\n",
            " Predicted  372   4.6373\n",
            " Predicted  373   F2374\n",
            " Predicted  374   6375\n",
            " Predicted  375   6.11376\n",
            " Predicted  376   5377\n",
            " Predicted  377   6.11378\n",
            " Predicted  378   6379\n",
            " Predicted  379   2.3.7.10380\n",
            " Predicted  380   6.11381\n",
            " Predicted  381   4.6382\n",
            " Predicted  382   B-2383\n",
            " Predicted  383   1.6384\n",
            " Predicted  384   B-2385\n",
            " Predicted  385   6.11386\n",
            " Predicted  386   F2387\n",
            " Predicted  387   4.6388\n",
            " Predicted  388   6389\n",
            " Predicted  389   6.11390\n",
            " Predicted  390   6391\n",
            " Predicted  391   E-2392\n",
            " Predicted  392   6393\n",
            " Predicted  393   4.6394\n",
            " Predicted  394   4.6395\n",
            " Predicted  395   A4396\n",
            " Predicted  396   5.10397\n",
            " Predicted  397   6.11398\n",
            " Predicted  398   A4399\n",
            " Predicted  399   6400\n",
            " Predicted  400   F2401\n",
            " Predicted  401   4.6402\n",
            " Predicted  402   5.9.0403\n",
            " Predicted  403   4.6404\n",
            " Predicted  404   5.9.0405\n",
            " Predicted  405   C5406\n",
            " Predicted  406   2.3.7.9407\n",
            " Predicted  407   6.11408\n",
            " Predicted  408   B-2409\n",
            " Predicted  409   D5410\n",
            " Predicted  410   A4411\n",
            " Predicted  411   4.6412\n",
            " Predicted  412   4.6413\n",
            " Predicted  413   5.9.0414\n",
            " Predicted  414   6.11415\n",
            " Predicted  415   3.9416\n",
            " Predicted  416   6.11417\n",
            " Predicted  417   F2418\n",
            " Predicted  418   F2419\n",
            " Predicted  419   4.6420\n",
            " Predicted  420   F2421\n",
            " Predicted  421   C5422\n",
            " Predicted  422   5.9.0423\n",
            " Predicted  423   6424\n",
            " Predicted  424   D5425\n",
            " Predicted  425   6.11426\n",
            " Predicted  426   4.6427\n",
            " Predicted  427   6.11428\n",
            " Predicted  428   6.10429\n",
            " Predicted  429   6.10.11430\n",
            " Predicted  430   6431\n",
            " Predicted  431   10.11.3.6432\n",
            " Predicted  432   B-2433\n",
            " Predicted  433   6434\n",
            " Predicted  434   D5435\n",
            " Predicted  435   6.11436\n",
            " Predicted  436   4.6437\n",
            " Predicted  437   4.6438\n",
            " Predicted  438   A4439\n",
            " Predicted  439   F3440\n",
            " Predicted  440   A4441\n",
            " Predicted  441   F2442\n",
            " Predicted  442   6443\n",
            " Predicted  443   5.7.9.0444\n",
            " Predicted  444   B-1445\n",
            " Predicted  445   6.11446\n",
            " Predicted  446   F2447\n",
            " Predicted  447   6.11448\n",
            " Predicted  448   B-2449\n",
            " Predicted  449   6450\n",
            " Predicted  450   6.11451\n",
            " Predicted  451   6452\n",
            " Predicted  452   6453\n",
            " Predicted  453   6.11454\n",
            " Predicted  454   6.10455\n",
            " Predicted  455   6.11456\n",
            " Predicted  456   6457\n",
            " Predicted  457   D5458\n",
            " Predicted  458   0.5459\n",
            " Predicted  459   5.7.9.0460\n",
            " Predicted  460   6.11461\n",
            " Predicted  461   A4462\n",
            " Predicted  462   5.9.0463\n",
            " Predicted  463   6.11464\n",
            " Predicted  464   B-2465\n",
            " Predicted  465   B-1466\n",
            " Predicted  466   4.6467\n",
            " Predicted  467   E-5468\n",
            " Predicted  468   F4469\n",
            " Predicted  469   6.11470\n",
            " Predicted  470   4.6471\n",
            " Predicted  471   F2472\n",
            " Predicted  472   C5473\n",
            " Predicted  473   E-3474\n",
            " Predicted  474   6475\n",
            " Predicted  475   4.6476\n",
            " Predicted  476   F2477\n",
            " Predicted  477   6.11478\n",
            " Predicted  478   5479\n",
            " Predicted  479   6.10480\n",
            " Predicted  480   6481\n",
            " Predicted  481   D5482\n",
            " Predicted  482   5.10483\n",
            " Predicted  483   6.11484\n",
            " Predicted  484   F2485\n",
            " Predicted  485   A4486\n",
            " Predicted  486   B-2487\n",
            " Predicted  487   6488\n",
            " Predicted  488   10.11.3.6489\n",
            " Predicted  489   6.11490\n",
            " Predicted  490   10.2.5491\n",
            " Predicted  491   6492\n",
            " Predicted  492   6.11493\n",
            " Predicted  493   6494\n",
            " Predicted  494   F2495\n",
            " Predicted  495   4.6496\n",
            " Predicted  496   B-1497\n",
            " Predicted  497   10.2.5498\n",
            " Predicted  498   B-2499\n",
            " Predicted  499   B-1"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lp5WcPlghe5"
      },
      "source": [
        "The last step is creating a MIDI file from the predictions.\n",
        "\n",
        "**music21** will help us again for this task. We should create a **Stream** and add to it the predicted notes and chords.\n",
        "\n",
        "We are adding an offset of 0.5 between elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7xYCPULXwV-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61d7878a-ade3-4adc-cff1-d3aa47090bdd"
      },
      "source": [
        "offset = 0\n",
        "output_notes = []\n",
        "# create note and chord objects based on the values generated by the model\n",
        "for pattern in prediction_output:\n",
        "    # pattern is a chord\n",
        "    if ('.' in pattern) or pattern.isdigit():\n",
        "        notes_in_chord = pattern.split('.')\n",
        "        notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            new_note = note.Note(int(current_note))\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            notes.append(new_note)\n",
        "        new_chord = chord.Chord(notes)\n",
        "        new_chord.offset = offset\n",
        "        output_notes.append(new_chord)\n",
        "    # pattern is a note\n",
        "    else:\n",
        "        new_note = note.Note(pattern)\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = instrument.Piano()\n",
        "        output_notes.append(new_note)\n",
        "\n",
        "    # increase offset each iteration so that notes do not stack\n",
        "    offset += 0.5\n",
        "\n",
        "midi_stream = stream.Stream(output_notes)\n",
        "midi_stream.write('midi', fp='test_output.mid')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'test_output.mid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}