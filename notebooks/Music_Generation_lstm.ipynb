{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Music_Generation_lstm.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPFAhMRSzgT0/MfEEEmB5FB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniSorice/Deep_Music_Generator/blob/main/notebooks/Music_Generation_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gJ3GwlYmvNT"
      },
      "source": [
        "# LSTM Music Generator "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNwQUwLIm7oW"
      },
      "source": [
        "\n",
        "\n",
        "In this notebook, we use an LSTM to generate some music.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFVGA5D4azA4"
      },
      "source": [
        "**This notebook was inspired (and part of the code comes from it) by [Music_Generation_LSTM](https://colab.research.google.com/drive/19TQqekOlnOSW36VCL8CPVEQKBBukmaEQ#scrollTo=DDOBVWULXfpz)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtmj0efInB_L"
      },
      "source": [
        "\n",
        "\n",
        "**Load dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USZpREpD6VTY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, SpatialDropout1D, LSTM, Activation, Bidirectional, Flatten, Attention\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import os\n",
        "from sklearn.metrics import roc_auc_score \n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import pickle\n",
        "from music21 import converter, instrument, stream, note, chord"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwF2wx4oRZXW"
      },
      "source": [
        "# Set to false if you are not running\n",
        "# this notebook in Google Colaboratory\n",
        "run_on_colab = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2w9a2MknGmP"
      },
      "source": [
        "**Set hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnnLeO8Emsx3"
      },
      "source": [
        "# output directory name:\n",
        "output_dir = 'model_output/LSTM'\n",
        "\n",
        "# training:\n",
        "epochs = 100\n",
        "batch_size = 64\n",
        "\n",
        "# vector-space embedding: \n",
        "n_dim = 64 \n",
        "sequence_length = 100\n",
        "\n",
        "# LSTM layer architecture:\n",
        "n1_lstm = 256 \n",
        "drop1_lstm = 0.2\n",
        "n2_lstm = 256 \n",
        "drop2_lstm = 0.2\n",
        "n3_lstm = 256 \n",
        "drop3_lstm = 0.2\n",
        "\n",
        "# dense layer architecture: \n",
        "n_dense = 128\n",
        "drop_dense = 0.2"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBjPl0cbI0_"
      },
      "source": [
        "**Google drive configuration (only Colab)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKil2p6sM2gZ",
        "outputId": "d94143a6-d9c9-4e7e-b1dd-fac5ba5aa2b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if(run_on_colab):\n",
        "  from google.colab import drive\n",
        "  # This will prompt for authorization.\n",
        "  drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3gQinYmutQ"
      },
      "source": [
        "**Load data** \\\\\n",
        "Original MIDI files\n",
        " I have obtained  **MIDI files** from [The Lakh MIDI Dataset v0.1](https://colinraffel.com/projects/lmd/). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wavgJZNpcR_f"
      },
      "source": [
        "## Processing data\n",
        "\n",
        "Let's process the files, and load them into **music21**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vILKBwYIcOvX",
        "outputId": "64e7d1f6-cce0-4f1d-aedb-da835ea2fb41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file = \"/content/drive/My Drive/ISPR_project/midi_songs/Andra tutto bene ('58).1.mid\"\n",
        "midi = converter.parse(file)\n",
        "notes_to_parse = midi.flat.notes\n",
        "for element in notes_to_parse[:10]:\n",
        "  print(element, element.offset)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<music21.chord.Chord F3 F2> 4.0\n",
            "<music21.note.Note A> 4.0\n",
            "<music21.chord.Chord B1 F#3 F#2> 4.0\n",
            "<music21.note.Note F> 4.0\n",
            "<music21.chord.Chord C4 F4> 4.0\n",
            "<music21.chord.Chord F#3 C#6 F#2> 4.5\n",
            "<music21.note.Note C#> 4.75\n",
            "<music21.chord.Chord F#2 E2 F#3> 5.0\n",
            "<music21.chord.Chord A4 A3 F4 C4 A3> 5.0\n",
            "<music21.note.Note F> 5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjyrXfKic2kI"
      },
      "source": [
        "I will process all MIDI files obtaining data from each note of chord.\n",
        "\n",
        "- If I process a **note**, I will store in the list a string representing the pitch (the note name) and the octave.\n",
        "\n",
        "- If I process a **chord** (Remember that chords are set of notes that are played at the same time) I will store a different type of string with numbers separated by dots. Each number represents the pitch of a chord note. \n",
        "\n",
        "As you can see, **I are not considering yet time offsets of each element**. In this first version, we won't consider them, so all the notes and chords will have the same duration. Maybe, in the future, I will consider them.\n",
        "\n",
        "I are creating a big list with all the elements of all the compositions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iawbQHYVTOFM",
        "outputId": "bd270837-ed13-4102-8207-86f4a2686f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "notes = []\n",
        "for i,file in enumerate(glob.glob(\"/content/drive/My Drive/ISPR_project/midi_songs/*.mid\")):\n",
        "  midi = converter.parse(file)\n",
        "  print('\\r', 'Parsing file ', i, \" \",file, end='')\n",
        "  notes_to_parse = None\n",
        "  try: # file has instrument parts\n",
        "    s2 = instrument.partitionByInstrument(midi)\n",
        "    notes_to_parse = s2.parts[0].recurse() \n",
        "  except: # file has notes in a flat structure\n",
        "    notes_to_parse = midi.flat.notes\n",
        "  for element in notes_to_parse:\n",
        "    if isinstance(element, note.Note):\n",
        "      notes.append(str(element.pitch))\n",
        "    elif isinstance(element, chord.Chord):\n",
        "      notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "with open('notes', 'wb') as filepath:\n",
        "  pickle.dump(notes, filepath)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Parsing file  3   /content/drive/My Drive/ISPR_project/midi_songs/Andra tutto bene ('58).1.mid"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuHePM_3dMXM"
      },
      "source": [
        "I obtain the number of different notes in our dataset, because this will be the **number of possible output classes**  of our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEiySfEnTzyz",
        "outputId": "13950bc7-3119-461f-fcc3-9c03a0ecb3be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Count different possible outputs\n",
        "n_vocab = (len(set(notes)))\n",
        "n_vocab"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-mABhgUnTcV"
      },
      "source": [
        "**Preprocess data** \\\\\n",
        "Now, there is some **data processing** that I have to do:\n",
        "\n",
        "- I will map each pitch or chord to an integer\n",
        "- I will create pairs of input sequences and its corresponding output note\n",
        "\n",
        "I can try different **sequence_length** to obtain different results. In this first version, I will use a sequence_length of 100.\n",
        "\n",
        "The network will made its prediction of the next note (or chord), based on the previous *sequence_length* notes (or chords). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4PpfIFRnWIA"
      },
      "source": [
        "# get all pitch names\n",
        "pitchnames = sorted(set(item for item in notes))\n",
        "# create a dictionary to map pitches to integers\n",
        "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "network_input = []\n",
        "network_output = []\n",
        "# create input sequences and the corresponding outputs\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "  sequence_in = notes[i:i + sequence_length] # Size sequence_length\n",
        "  sequence_out = notes[i + sequence_length]  # Size 1\n",
        "  # Map pitches of sequence_in to integers\n",
        "  network_input.append([note_to_int[char] for char in sequence_in])\n",
        "  # Map integer of sequence_out to an integer\n",
        "  network_output.append(note_to_int[sequence_out])\n",
        "n_patterns = len(network_input)\n",
        "# reshape the input into a format compatible with LSTM layers\n",
        "network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "# normalize input\n",
        "network_input = network_input / float(n_vocab)\n",
        "network_output = utils.to_categorical(network_output)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVRPauXUdq5Y"
      },
      "source": [
        "Let's see the new metwork_input size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZbR7YflUdc_",
        "outputId": "638c0257-2eb4-4b27-8c39-c9394258c59f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "network_input.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4903, 100, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6--Wc8UXnVgY"
      },
      "source": [
        "**Design neural network architecture** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4ztPAQsnb7T"
      },
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        n1_lstm,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(Dropout(drop1_lstm))\n",
        "    model.add(LSTM(n2_lstm, return_sequences=True)) # Use True with more than one LSTM layer\n",
        "    model.add(Dropout(drop2_lstm))\n",
        "    model.add(LSTM(n3_lstm))\n",
        "    model.add(Dense(n_dense))\n",
        "    model.add(Dropout(drop_dense))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nsNVMocnhNC",
        "outputId": "e809e3fd-c7d9-42e2-f0d4-84baba2087a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = create_network(network_input,n_vocab)\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_9 (LSTM)                (None, 100, 256)          264192    \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (None, 100, 256)          525312    \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 100, 256)          0         \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 71)                9159      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 71)                0         \n",
            "=================================================================\n",
            "Total params: 1,356,871\n",
            "Trainable params: 1,356,871\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbO-XA3Dfrd2"
      },
      "source": [
        "In case we want to use previously trained weights, to continue the training in the point we left it, we should load them into the model.\n",
        "\n",
        "This is very useful in Google Colaboratory, that usually kills the virtual machine that is executing the Jupyter notework after a certime amount of time. If this happens to you, you should have to look for the last weights file in your configured Drive account and use it to train the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFbSthkLUwmx"
      },
      "source": [
        "# In case we want to use previously trained weights\n",
        "weights = \"\"\n",
        "if(len(weights)>0): model.load_weights(weights)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqyWj37PnfaS"
      },
      "source": [
        "**Configure model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiAxaVxlnpS5"
      },
      "source": [
        "filepath = \"/content/drive/My Drive/ISPR_project/LSTM{epoch:02d}-{loss:.4f}.h5\"\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss',verbose=0,\n",
        "                             save_best_only=True,mode='min')\n",
        "\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp2KgUt6nvNj"
      },
      "source": [
        "**Train!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNheAnB7nzZg",
        "outputId": "ae0f87ad-2f10-4dca-dafe-fae1c30f9316",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(network_input, network_output, epochs=epochs, batch_size=batch_size, callbacks=callbacks_list)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 3.1896\n",
            "Epoch 2/100\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 3.1208\n",
            "Epoch 3/100\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 3.0882\n",
            "Epoch 4/100\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 2.9737\n",
            "Epoch 5/100\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 2.7240\n",
            "Epoch 6/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 2.4570\n",
            "Epoch 7/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 2.3073\n",
            "Epoch 8/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 2.1258\n",
            "Epoch 9/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.9208\n",
            "Epoch 10/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.7391\n",
            "Epoch 11/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.6069\n",
            "Epoch 12/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.4617\n",
            "Epoch 13/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.3154\n",
            "Epoch 14/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.1865\n",
            "Epoch 15/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.0216\n",
            "Epoch 16/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.8979\n",
            "Epoch 17/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.7942\n",
            "Epoch 18/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.7137\n",
            "Epoch 19/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.6401\n",
            "Epoch 20/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.5430\n",
            "Epoch 21/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.4753\n",
            "Epoch 22/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.4036\n",
            "Epoch 23/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.3668\n",
            "Epoch 24/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.3348\n",
            "Epoch 25/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.2808\n",
            "Epoch 26/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.2575\n",
            "Epoch 27/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.3176\n",
            "Epoch 28/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.2150\n",
            "Epoch 29/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.2024\n",
            "Epoch 30/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.1781\n",
            "Epoch 31/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.1552\n",
            "Epoch 32/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.1603\n",
            "Epoch 33/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.1339\n",
            "Epoch 34/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.1205\n",
            "Epoch 35/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.1098\n",
            "Epoch 36/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.1077\n",
            "Epoch 37/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.1037\n",
            "Epoch 38/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 3.7253\n",
            "Epoch 39/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 3.5119\n",
            "Epoch 40/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 3.4074\n",
            "Epoch 41/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 3.3626\n",
            "Epoch 42/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.8397\n",
            "Epoch 43/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.1668\n",
            "Epoch 44/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.1046\n",
            "Epoch 45/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0922\n",
            "Epoch 46/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0869\n",
            "Epoch 47/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0774\n",
            "Epoch 48/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0821\n",
            "Epoch 49/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0732\n",
            "Epoch 50/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0616\n",
            "Epoch 51/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0696\n",
            "Epoch 52/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0708\n",
            "Epoch 53/100\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 0.0641\n",
            "Epoch 54/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0601\n",
            "Epoch 55/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0608\n",
            "Epoch 56/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0495\n",
            "Epoch 57/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0517\n",
            "Epoch 58/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0565\n",
            "Epoch 59/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0550\n",
            "Epoch 60/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0519\n",
            "Epoch 61/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0491\n",
            "Epoch 62/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0407\n",
            "Epoch 63/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0557\n",
            "Epoch 64/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0535\n",
            "Epoch 65/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0407\n",
            "Epoch 66/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0349\n",
            "Epoch 67/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0597\n",
            "Epoch 68/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0429\n",
            "Epoch 69/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0557\n",
            "Epoch 70/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0400\n",
            "Epoch 71/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0526\n",
            "Epoch 72/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0344\n",
            "Epoch 73/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0460\n",
            "Epoch 74/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0378\n",
            "Epoch 75/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0331\n",
            "Epoch 76/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0372\n",
            "Epoch 77/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0311\n",
            "Epoch 78/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0286\n",
            "Epoch 79/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0345\n",
            "Epoch 80/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0311\n",
            "Epoch 81/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0333\n",
            "Epoch 82/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0417\n",
            "Epoch 83/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0278\n",
            "Epoch 84/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0362\n",
            "Epoch 85/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0295\n",
            "Epoch 86/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0385\n",
            "Epoch 87/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0274\n",
            "Epoch 88/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0308\n",
            "Epoch 89/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0312\n",
            "Epoch 90/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0365\n",
            "Epoch 91/100\n",
            "77/77 [==============================] - 3s 33ms/step - loss: 0.0210\n",
            "Epoch 92/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0326\n",
            "Epoch 93/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0339\n",
            "Epoch 94/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0344\n",
            "Epoch 95/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0221\n",
            "Epoch 96/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0287\n",
            "Epoch 97/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0316\n",
            "Epoch 98/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0252\n",
            "Epoch 99/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0244\n",
            "Epoch 100/100\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 0.0266\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f981c888668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAgrzFyRn3uq"
      },
      "source": [
        "**Music generation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1CA_EQdoN5P"
      },
      "source": [
        "# In case we want to use other previously trained weights\n",
        "weights = \"path/to/weights\"\n",
        "if(len(weights)>0): model.load_weights(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WtYEXpyoPPK"
      },
      "source": [
        "# Generate network input again\n",
        "network_input = []\n",
        "output = []\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "  sequence_in = notes[i:i + sequence_length]\n",
        "  sequence_out = notes[i + sequence_length]\n",
        "  network_input.append([note_to_int[char] for char in sequence_in])\n",
        "  output.append(note_to_int[sequence_out])\n",
        "n_patterns = len(network_input)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuUGi5rVgbJQ"
      },
      "source": [
        "The workflow now is:\n",
        "\n",
        "\n",
        "1.   Pick a **seed sequence** randomly from your list of inputs (*pattern* variable)\n",
        "2.   Pass it as input for your model to generate a new element (note or chord)\n",
        "3.   Add the new element to your final song and to your *pattern* list\n",
        "4.   Remove the first item from *pattern*\n",
        "5.   Go to step 2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUTEiS0NXtkl",
        "outputId": "c32f8615-5c5c-49f8-9c98-82c5556dc12f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "# pick a random sequence from the input as a starting point for the prediction\n",
        "start = np.random.randint(0, len(network_input)-1)\n",
        "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "pattern = network_input[start]\n",
        "prediction_output = []\n",
        "# generate 500 notes\n",
        "for i,note_index in enumerate(range(500)):\n",
        "  prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "  prediction_input = prediction_input / float(n_vocab)\n",
        "  prediction = model.predict(prediction_input, verbose=0)\n",
        "  index = np.argmax(prediction)\n",
        "  result = int_to_note[index]\n",
        "  print('\\r', 'Predicted ', i, \" \",result, end='')\n",
        "  prediction_output.append(result)\n",
        "  pattern.append(index)\n",
        "  pattern = pattern[1:len(pattern)]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Predicted  499   B2"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lp5WcPlghe5"
      },
      "source": [
        "The last step is creating a MIDI file from the predictions.\n",
        "\n",
        "**music21** will help us again for this task. We should create a **Stream** and add to it the predicted notes and chords.\n",
        "\n",
        "We are adding an offset of 0.5 between elements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7xYCPULXwV-",
        "outputId": "6aadd88f-5fda-4a48-accc-bc87d7aea051",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "offset = 0\n",
        "output_notes = []\n",
        "# create note and chord objects based on the values generated by the model\n",
        "for pattern in prediction_output:\n",
        "    # pattern is a chord\n",
        "    if ('.' in pattern) or pattern.isdigit():\n",
        "        notes_in_chord = pattern.split('.')\n",
        "        notes = []\n",
        "        for current_note in notes_in_chord:\n",
        "            new_note = note.Note(int(current_note))\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            notes.append(new_note)\n",
        "        new_chord = chord.Chord(notes)\n",
        "        new_chord.offset = offset\n",
        "        output_notes.append(new_chord)\n",
        "    # pattern is a note\n",
        "    else:\n",
        "        new_note = note.Note(pattern)\n",
        "        new_note.offset = offset\n",
        "        new_note.storedInstrument = instrument.Piano()\n",
        "        output_notes.append(new_note)\n",
        "\n",
        "    # increase offset each iteration so that notes do not stack\n",
        "    offset += 0.5\n",
        "\n",
        "midi_stream = stream.Stream(output_notes)\n",
        "midi_stream.write('midi', fp='test_output.mid')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'test_output.mid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}