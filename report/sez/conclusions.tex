
\begin{thebibliography}{9}
	
	\bibitem{Iannis} 
	Iannis Xenakis
	\\\textit{Formalized Music: Thought and Mathematics in Composition}, Indiana University Press, 1963. 
	
	\bibitem{AlexNet} 
	A. Krizhevsky, I. Sutskever, and G. E. Hinton.
	\\\textit{ImageNet classification with deep convolutional neural networks}, Advances in Neural Information Processing Systems, NIPS, 2012. 
	\\\texttt{http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf}
	
	\bibitem{lakh} 
	Colin Raffel
	\textit{The Lakh MIDI Dataset v0.1}. \\
	\textit{ "Learning-Based Methods for Comparing Sequences, with Applications to Audio-to-MIDI Alignment and Matching"}. PhD Thesis, 2016.
	\\\texttt{https://colinraffel.com/projects/lmd/}
	
	\bibitem{AIAYN} 
	A. Vaswan et al.
	\\\textit{Attention Is All You Need}, NIPS 2017. 
	\\\texttt{https://arxiv.org/pdf/1706.03762.pdf}

	\bibitem{tranformerxl} 
	Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
	\textit{Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, arXiv 2019. 
	\\\texttt{https://arxiv.org/pdf/1901.02860.pdf}

	\bibitem{raecompressive2019} 
	Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M andHillier, Chloe and Lillicrap, Timothy P \\
	\textit{Compressive Transformers for Long-Range Sequence Modelling}, arXiv 2019. 
	\\\texttt{https://arxiv.org/abs/1911.05507}
	 
	\bibitem{compressive_library} 
	Phil Wang
	\\\textit{Compressive Transformer in Pytorch}.
	\\\texttt{https://github.com/lucidrains/compressive-transformer-pytorch}
	
	\bibitem{music_survey} 
	Jean-Pierre Briot,1, Ga\"etan Hadjeres†and Fran\c{c}ois-David Pachet
	\\\textit{Deep Learning Techniques for Music Generation– A Survey}.
	\\\texttt{https://arxiv.org/pdf/1709.01620.pdf}

	\bibitem{eff_transf} 
	Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya.
	\\\textit{The Efficient Transformer}.
	\\\texttt{https://arxiv.org/pdf/1709.01620.pdf}
	
	\bibitem{transformer_autoencoders} 
	Kristy Choi, Curtis Hawthorne, Ian Simon, Monica Dinculescu, Jesse Engel.
	\\\textit{Encoding Musical Style with Transformer Autoencoders}.
	\\\texttt{https://arxiv.org/pdf/1912.05537.pdf}
\end{thebibliography}



